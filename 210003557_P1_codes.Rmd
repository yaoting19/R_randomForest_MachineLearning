---
title: "210003557_P1_codes"
author: '210003557'
date: '2022-03-15'
output: html_document
---
Load the library and dataframe
```{r}
library(tidyverse)
library(randomForest)
library(ISLR2)
df <- read.csv(file = "D:/OneDrive - University of St Andrews/ID5059_Dataset/archive/vehicles.csv")
```


Right now, I will start cleaning the dataframe since there are too many weird values

I will start by selecting 4 independent variables + 1 dependent variable: price (according to the specification)
```{r}
select1 <- select(df,price,year,manufacturer,condition,odometer)
```


Next, I will filter the rows with empty values out
```{r}
vehicles <- filter(select1, price!="", year!="", manufacturer!="", condition!="",odometer!="")
```


Next, loop into the data frame, classify normal rows(as 1) and weird rows(as 0)
```{r}
for (i in 1:nrow(vehicles)) {
  if(nchar(vehicles$year[i]==4) & vehicles$condition[i]=="excellent" | vehicles$condition[i]=="fair" | vehicles$condition[i]=="good" | vehicles$condition[i]=="like new" | vehicles$condition[i]=="new" | vehicles$condition[i]=="salvage") {
    vehicles$boolean[i] <- 1
  } else {
    vehicles$boolean[i] <- 0
  }
}
```


Choose the normal rows
```{r}
vehicles <- filter(vehicles,boolean ==1, price!=0)
```


Transfer price and odometer columns into numerical values
```{r}
vehicles$price <- as.numeric(vehicles$price, vehicles$odometer)
```


Delete the low-end outliers for price & the Boolean column. The reason why I deleted the low-end ones was because I did some research online and the cheapest used cars were around $2,000 so I deleted the ones with a price < 2,000
```{r}
vehicles <- vehicles[!(vehicles$price < 2000),]
vehicles <- vehicles[,-6]
View(vehicles)
```


Now, I will split data set into train/test data set
```{r}
set.seed(3557)
dataSplit <- sample(2,nrow(vehicles), replace = T, prob = c(0.8,0.2))
train <- vehicles[dataSplit==1,]
test <- vehicles[dataSplit==2,]
```


Since the data set is too big for my laptop to run, I create a sub training data set
```{r}
set.seed(3557)
subset <- sample(1:nrow(train), round(0.1*nrow(train)))
train.subset <- vehicles[subset,]
```


Let's check the outliers again before creating random forest. I choose not to delete the high-end outliers because the prices are still reasonable
```{r}
boxplot(train.subset$price)
```


Generate random forest (using train.subset), the default number of trees is 500
```{r}
model.500 <- randomForest(price ~., data=train.subset, importance = T, proximity = TRUE)
model.500
plot(model.500)
importance(model.500)
```


As the error seems a bit large, I will try to generate 1000 trees
```{r}
model.1000 <- randomForest(price ~., data=train.subset, importance = T, proximity = TRUE, ntree=1000)
model.1000
plot(model.1000)
importance(model.1000)
```
According to the plot, the year column and the odometer column have more influence to the price, and 900 trees seems to be relatively ideal.


Determining mtry (number of covariate considered). According to the result, mtry = 1 provides the best result
```{r}
features <- setdiff(x=names(train.subset),y="price")
set.seed(3557)
tuneRF(x=train.subset[features],y=train.subset$price,mtryStart=1,ntreeTry=900)
```


Now let's try different complexity above and predict MSE using test data set. I will generate 3 models to prove my selection is correct.

model.optimal: ntree = 900 mtry = 1
```{r}
model.optimal <- randomForest (price ~., data=train.subset, importance = T, proximity = TRUE, mtry = 1, ntree = 900)
predict.optimal <- predict(model.optimal,test)
mean.optimal <- mean((predict.optimal-test$price)^2)
```


model.500: ntree = 500 mtry = 1 (which is the "model.500" variable)
```{r}
predict.candidate1 <- predict(model.500,test)
mean.candidate1 <- mean((predict.candidate1-test$price)^2)
```


model.500: ntree = 1000 mtry = 1 (which is the "model.1000" variable)
```{r}
predict.candidate2 <- predict(model.1000,test)
mean.candidate2 <- mean((predict.candidate2-test$price)^2)
```


Test the result
```{r}
mean.optimal
mean.candidate1
mean.candidate2
mean.optimal < mean.candidate1 & mean.optimal < mean.candidate2
```
The result shows that mean.optimal (ntree = 900, mtry = 1) has the least MSE on test data set, which means it has the lowest generalisation error, making it the most ideal model in this scenario.